{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Keyword-extraction.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPR9ZpxfiK7JOm91VqdR4Nm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prajwal0210/keyword-extraction/blob/main/Keyword_extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Z8Wb_zhokpp",
        "outputId": "d60a23c6-c1e1-4149-b833-4aebec11876a"
      },
      "source": [
        "# Import libraries for text preprocessing\n",
        "import re\n",
        "import nltk\n",
        "\n",
        "\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICHjkjeErtDK"
      },
      "source": [
        "# **Pre-process the dataset to get a cleaned, normalized text**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1oBVlQOMomtX"
      },
      "source": [
        "paragraph = \"\"\"Thank you all so very much. Thank you to the Academy. \n",
        "               Thank you to all of you in this room. I have to congratulate \n",
        "               the other incredible nominees this year. The Revenant was \n",
        "               the product of the tireless efforts of an unbelievable cast\n",
        "               and crew. First off, to my brother in this endeavor, Mr. Tom \n",
        "               Hardy. Tom, your talent on screen can only be surpassed by \n",
        "               your friendship off screen … thank you for creating a t\n",
        "               ranscendent cinematic experience. Thank you to everybody at \n",
        "               Fox and New Regency … my entire team. I have to thank \n",
        "               everyone from the very onset of my career … To my parents; \n",
        "               none of this would be possible without you. And to my \n",
        "               friends, I love you dearly; you know who you are. And lastly,\n",
        "               I just want to say this: Making The Revenant was about\n",
        "               man's relationship to the natural world. A world that we\n",
        "               collectively felt in 2015 as the hottest year in recorded\n",
        "               history. Our production needed to move to the southern\n",
        "               tip of this planet just to be able to find snow. Climate\n",
        "               change is real, it is happening right now. It is the most\n",
        "               urgent threat facing our entire species, and we need to work\n",
        "               collectively together and stop procrastinating. We need to\n",
        "               support leaders around the world who do not speak for the \n",
        "               big polluters, but who speak for all of humanity, for the\n",
        "               indigenous people of the world, for the billions and \n",
        "               billions of underprivileged people out there who would be\n",
        "               most affected by this. For our children’s children, and \n",
        "               for those people out there whose voices have been drowned\n",
        "               out by the politics of greed. I thank you all for this \n",
        "               amazing award tonight. Let us not take this planet for \n",
        "               granted. I do not take tonight for granted. Thank you so very much.\"\"\""
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tz1T91wQ4I6M",
        "outputId": "0697780b-e971-4892-e146-7f84feedc6cb"
      },
      "source": [
        "print(keywords(paragraph, words=30, lemmatize=True))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "thank\n",
            "collectively\n",
            "climate\n",
            "entire\n",
            "people\n",
            "threat\n",
            "tonight\n",
            "award\n",
            "cinematic\n",
            "experience\n",
            "unbelievable cast\n",
            "tireless efforts\n",
            "happening right\n",
            "recorded\n",
            "history\n",
            "big\n",
            "polluters\n",
            "procrastinating\n",
            "stop\n",
            "tom\n",
            "hardy\n",
            "production\n",
            "world\n",
            "need\n",
            "incredible nominees\n",
            "natural\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}